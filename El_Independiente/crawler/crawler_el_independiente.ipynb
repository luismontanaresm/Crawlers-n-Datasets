{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler del sitio Reclamos.cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(page):\n",
    "    url = f\"https://www.elperiodista.cl/page/{page}/\"\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_claims_urls(s):\n",
    "    content_column = s.find(\"div\", {\"class\" : \"content-column\"})\n",
    "    \n",
    "    content_column_posts = content_column.find_all(\"a\", {\"class\" : \"post-url post-title\"})\n",
    "    posts_urls = list()\n",
    "    for p in content_column_posts:\n",
    "        posts_urls.append(p.get(\"href\"))\n",
    "    return posts_urls\n",
    "def get_reqs(s):\n",
    "    return [requests.get(url) for url in get_claims_urls(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_req(req, verbose = False):\n",
    "    s = BeautifulSoup(req.text)\n",
    "    headline = s.find(\"span\",{\"class\":\"post-title\"}).text.strip().replace(\"\\n\",\"\")\n",
    "    lead = s.find(\"h2\",{\"class\":\"post-subtitle\"}).text.strip().replace(\"\\n\",\"\")\n",
    "    news_type = s.find_all(\"li\", {\"class\":\"bf-breadcrumb-item\"})[1].find(\"a\").text.strip().replace(\"\\n\",\"\")\n",
    "    datetime = s.find(\"time\",{\"class\":\"post-published updated\"}).get(\"datetime\").strip().replace(\"\\n\",\"\")\n",
    "    body = s.find(\"div\",{\"class\":\"entry-content\"}).text.replace(\"Continuar leyendo\", \"\").strip().replace(\"\\n\",\"\")\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(\"Headline:\",headline)\n",
    "        print(\"Lead:\",lead)\n",
    "        print(\"News type:\", news_type)\n",
    "        print(\"Datetime:\",datetime)\n",
    "        print(\"Body\",body)\n",
    "    \n",
    "    return {\n",
    "        \"headline\" : headline,\n",
    "        \"lead\": lead,\n",
    "        \"news_type\": news_type,\n",
    "        \"datetime\": datetime,\n",
    "        \"body\": body\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def crawl_reclamos(start_page, end_page, filename = 'noticias.csv', step=1):\n",
    "    list_dic = []\n",
    "    for i in range(start_page, end_page + 1,step):\n",
    "        s = get_soup(i)\n",
    "        time.sleep(0.01)\n",
    "        reqs = False\n",
    "        try:\n",
    "            reqs = get_reqs(s)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if reqs:\n",
    "            for req in reqs:\n",
    "                try:\n",
    "                    list_dic.append(crawl_req(req, False))\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "    df = pd.DataFrame(list_dic)\n",
    "    df.to_csv(filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 14s, sys: 1.25 s, total: 1min 15s\n",
      "Wall time: 17min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = crawl_reclamos(2,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threading scrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of maximum threads: 8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import threading\n",
    "print(\"Number of maximum threads:\", multiprocessing.cpu_count())\n",
    "\n",
    "MUTEX = threading.Lock()\n",
    "FILENAME = \"threading_noticias.csv\"\n",
    "\n",
    "def append_crawling_req(req):\n",
    "    df = crawl_req(req)\n",
    "    df.to_csv(FILENAME, mode='a', header=False)\n",
    "\n",
    "def crawl_with_threading(start_page, end_page, filename = 'noticias.csv', step=1):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
